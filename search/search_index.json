{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"rustmolbpe","text":"<p>A high-performance BPE (Byte Pair Encoding) tokenizer for molecular SMILES written in Rust with Python bindings.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>SMILES-aware tokenization: Correctly handles multi-character atoms (Br, Cl), bracket atoms ([C@@H], [N+]), ring closures, and stereochemistry</li> <li>Fast training: Parallel processing with Rayon for efficient training on large molecular datasets</li> <li>Streaming support: Train on datasets of any size with configurable buffer sizes</li> <li>Special tokens: Built-in PAD, UNK, BOS, EOS tokens for sequence modeling</li> <li>Batch padding: Ready for transformer models with attention masks</li> <li>SMILESPE compatibility: Load and save vocabularies in SMILESPE format</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>rustmolbpe is significantly faster than the original Python SMILESPE implementation:</p> Operation Speedup Encoding 25-35x faster Training 16-18x faster"},{"location":"#throughput","title":"Throughput","text":"<ul> <li>Batch encoding: ~200,000-280,000 SMILES/second</li> <li>Training: 2.8M molecules in ~100 seconds</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#from-pypi-coming-soon","title":"From PyPI (coming soon)","text":"<pre><code>pip install rustmolbpe\n</code></pre>"},{"location":"#from-source","title":"From source","text":"<pre><code># Install maturin\npip install maturin\n\n# Clone and build\ngit clone https://github.com/HFooladi/rustmolbpe.git\ncd rustmolbpe\nmaturin develop --release\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import rustmolbpe\n\n# Create tokenizer and load vocabulary\ntokenizer = rustmolbpe.SmilesTokenizer()\ntokenizer.load_vocabulary(\"data/chembl36_vocab.txt\")\n\n# Encode SMILES\nids = tokenizer.encode(\"CC(=O)Nc1ccc(O)cc1\")  # paracetamol\nprint(ids)  # [2864, 1077]\n\n# Decode back\nsmiles = tokenizer.decode(ids)\nprint(smiles)  # CC(=O)Nc1ccc(O)cc1\n\n# Batch processing with padding (for ML)\nresult = tokenizer.encode_batch_padded(\n    [\"CCO\", \"c1ccccc1\", \"CC(=O)O\"],\n    add_special_tokens=True,\n    return_attention_mask=True\n)\nprint(result[\"input_ids\"])\nprint(result[\"attention_mask\"])\n</code></pre>"},{"location":"#pre-trained-vocabularies","title":"Pre-trained Vocabularies","text":"<p>Pre-trained vocabularies are included:</p> <ul> <li><code>data/chembl36_vocab.txt</code> - Trained on ChEMBL 36 (2.8M drug-like molecules)</li> <li><code>data/pubchem_10M_vocab.txt</code> - Trained on PubChem (10M diverse molecules)</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#module-rustmolbpe","title":"Module: rustmolbpe","text":""},{"location":"api/#atomwise_tokenize","title":"atomwise_tokenize","text":"<pre><code>def atomwise_tokenize(smiles: str) -&gt; List[str]\n</code></pre> <p>Tokenize a SMILES string into atom-level tokens.</p> <p>Arguments:</p> <ul> <li><code>smiles</code> (str): SMILES string to tokenize</li> </ul> <p>Returns:</p> <ul> <li><code>List[str]</code>: List of atom-level tokens</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rustmolbpe.atomwise_tokenize(\"CCO\")\n['C', 'C', 'O']\n\n&gt;&gt;&gt; rustmolbpe.atomwise_tokenize(\"c1ccccc1\")\n['c', '1', 'c', 'c', 'c', 'c', 'c', '1']\n\n&gt;&gt;&gt; rustmolbpe.atomwise_tokenize(\"[C@@H](O)C\")\n['[C@@H]', '(', 'O', ')', 'C']\n\n&gt;&gt;&gt; rustmolbpe.atomwise_tokenize(\"CBr\")\n['C', 'Br']\n</code></pre>"},{"location":"api/#class-smilestokenizer","title":"Class: SmilesTokenizer","text":"<p>BPE tokenizer for molecular SMILES strings.</p>"},{"location":"api/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None\n</code></pre> <p>Create a new tokenizer with special tokens initialized.</p> <p>Special tokens are always at fixed IDs:</p> Token ID String PAD 0 <code>&lt;pad&gt;</code> UNK 1 <code>&lt;unk&gt;</code> BOS 2 <code>&lt;bos&gt;</code> EOS 3 <code>&lt;eos&gt;</code>"},{"location":"api/#training-methods","title":"Training Methods","text":""},{"location":"api/#train_from_iterator","title":"train_from_iterator","text":"<pre><code>def train_from_iterator(\n    self,\n    iterator: Iterator[str],\n    vocab_size: int,\n    buffer_size: int = 8192,\n    min_frequency: int = 2\n) -&gt; None\n</code></pre> <p>Train the tokenizer from a SMILES iterator.</p> <p>Arguments:</p> <ul> <li><code>iterator</code> (Iterator[str]): Iterator yielding SMILES strings</li> <li><code>vocab_size</code> (int): Target vocabulary size (including special tokens and base atoms)</li> <li><code>buffer_size</code> (int, optional): Number of SMILES to buffer for parallel processing. Default: 8192</li> <li><code>min_frequency</code> (int, optional): Minimum frequency for a pair to be merged. Default: 2</li> </ul> <p>Example:</p> <pre><code>def smiles_generator(path):\n    with open(path) as f:\n        for line in f:\n            yield line.strip()\n\ntokenizer = rustmolbpe.SmilesTokenizer()\ntokenizer.train_from_iterator(\n    smiles_generator(\"molecules.smi\"),\n    vocab_size=8000\n)\n</code></pre>"},{"location":"api/#vocabulary-io","title":"Vocabulary I/O","text":""},{"location":"api/#load_vocabulary","title":"load_vocabulary","text":"<pre><code>def load_vocabulary(self, path: str) -&gt; None\n</code></pre> <p>Load vocabulary from a SMILESPE-format file.</p> <p>Arguments:</p> <ul> <li><code>path</code> (str): Path to vocabulary file</li> </ul> <p>Raises:</p> <ul> <li><code>IOError</code>: If file cannot be read</li> </ul>"},{"location":"api/#save_vocabulary","title":"save_vocabulary","text":"<pre><code>def save_vocabulary(self, path: str) -&gt; None\n</code></pre> <p>Save vocabulary to a SMILESPE-format file.</p> <p>Arguments:</p> <ul> <li><code>path</code> (str): Path to save vocabulary file</li> </ul> <p>Raises:</p> <ul> <li><code>IOError</code>: If file cannot be written</li> </ul>"},{"location":"api/#encoding-methods","title":"Encoding Methods","text":""},{"location":"api/#encode","title":"encode","text":"<pre><code>def encode(self, smiles: str, add_special_tokens: bool = False) -&gt; List[int]\n</code></pre> <p>Encode a SMILES string to token IDs.</p> <p>Arguments:</p> <ul> <li><code>smiles</code> (str): SMILES string to encode</li> <li><code>add_special_tokens</code> (bool, optional): If True, add BOS at start and EOS at end. Default: False</li> </ul> <p>Returns:</p> <ul> <li><code>List[int]</code>: List of token IDs</li> </ul> <p>Example:</p> <pre><code>ids = tokenizer.encode(\"CCO\")  # [42]\nids = tokenizer.encode(\"CCO\", add_special_tokens=True)  # [2, 42, 3]\n</code></pre>"},{"location":"api/#batch_encode","title":"batch_encode","text":"<pre><code>def batch_encode(\n    self,\n    smiles_list: List[str],\n    add_special_tokens: bool = False\n) -&gt; List[List[int]]\n</code></pre> <p>Encode multiple SMILES strings in parallel.</p> <p>Arguments:</p> <ul> <li><code>smiles_list</code> (List[str]): List of SMILES strings</li> <li><code>add_special_tokens</code> (bool, optional): If True, add BOS/EOS tokens. Default: False</li> </ul> <p>Returns:</p> <ul> <li><code>List[List[int]]</code>: List of token ID lists</li> </ul>"},{"location":"api/#decoding-methods","title":"Decoding Methods","text":""},{"location":"api/#decode","title":"decode","text":"<pre><code>def decode(self, ids: List[int]) -&gt; str\n</code></pre> <p>Decode token IDs back to a SMILES string.</p> <p>Arguments:</p> <ul> <li><code>ids</code> (List[int]): List of token IDs</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Decoded SMILES string</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If an ID is not in vocabulary</li> </ul>"},{"location":"api/#batch_decode","title":"batch_decode","text":"<pre><code>def batch_decode(self, ids_list: List[List[int]]) -&gt; List[str]\n</code></pre> <p>Decode multiple token sequences in parallel.</p> <p>Arguments:</p> <ul> <li><code>ids_list</code> (List[List[int]]): List of token ID lists</li> </ul> <p>Returns:</p> <ul> <li><code>List[str]</code>: List of decoded SMILES strings</li> </ul>"},{"location":"api/#padding-methods","title":"Padding Methods","text":""},{"location":"api/#pad","title":"pad","text":"<pre><code>def pad(\n    self,\n    sequences: List[List[int]],\n    max_length: Optional[int] = None,\n    padding: str = \"right\",\n    truncation: bool = False,\n    return_attention_mask: bool = True\n) -&gt; Dict[str, List[List[int]]]\n</code></pre> <p>Pad sequences to equal length.</p> <p>Arguments:</p> <ul> <li><code>sequences</code> (List[List[int]]): List of token ID sequences</li> <li><code>max_length</code> (int, optional): Target length. If None, uses longest sequence length</li> <li><code>padding</code> (str, optional): Padding side, either \"right\" or \"left\". Default: \"right\"</li> <li><code>truncation</code> (bool, optional): If True, truncate sequences longer than max_length. Default: False</li> <li><code>return_attention_mask</code> (bool, optional): If True, include attention_mask in result. Default: True</li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, List[List[int]]]</code>: Dictionary with keys:<ul> <li><code>\"input_ids\"</code>: Padded token ID sequences</li> <li><code>\"attention_mask\"</code>: Attention masks (if requested)</li> </ul> </li> </ul>"},{"location":"api/#encode_batch_padded","title":"encode_batch_padded","text":"<pre><code>def encode_batch_padded(\n    self,\n    smiles_list: List[str],\n    max_length: Optional[int] = None,\n    padding: str = \"right\",\n    truncation: bool = False,\n    add_special_tokens: bool = False,\n    return_attention_mask: bool = True\n) -&gt; Dict[str, List[List[int]]]\n</code></pre> <p>Encode multiple SMILES and pad to equal length.</p> <p>Convenience method combining <code>batch_encode</code> and <code>pad</code>.</p> <p>Arguments:</p> <ul> <li><code>smiles_list</code> (List[str]): List of SMILES strings</li> <li><code>max_length</code> (int, optional): Target length. If None, uses longest sequence length</li> <li><code>padding</code> (str, optional): Padding side, either \"right\" or \"left\". Default: \"right\"</li> <li><code>truncation</code> (bool, optional): If True, truncate sequences longer than max_length. Default: False</li> <li><code>add_special_tokens</code> (bool, optional): If True, add BOS/EOS tokens. Default: False</li> <li><code>return_attention_mask</code> (bool, optional): If True, include attention_mask in result. Default: True</li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, List[List[int]]]</code>: Dictionary with \"input_ids\" and optionally \"attention_mask\"</li> </ul> <p>Example:</p> <pre><code>result = tokenizer.encode_batch_padded(\n    [\"CCO\", \"c1ccccc1\"],\n    max_length=10,\n    add_special_tokens=True\n)\nprint(result[\"input_ids\"])       # [[2, 42, 3, 0, 0, ...], [2, 15, 3, 0, 0, ...]]\nprint(result[\"attention_mask\"])  # [[1, 1, 1, 0, 0, ...], [1, 1, 1, 0, 0, ...]]\n</code></pre>"},{"location":"api/#vocabulary-access","title":"Vocabulary Access","text":""},{"location":"api/#get_vocabulary","title":"get_vocabulary","text":"<pre><code>def get_vocabulary(self) -&gt; List[Tuple[str, int]]\n</code></pre> <p>Get vocabulary as (token, id) pairs.</p> <p>Returns:</p> <ul> <li><code>List[Tuple[str, int]]</code>: List of (token_string, token_id) tuples</li> </ul>"},{"location":"api/#id_to_token","title":"id_to_token","text":"<pre><code>def id_to_token(self, id: int) -&gt; str\n</code></pre> <p>Convert token ID to token string.</p> <p>Arguments:</p> <ul> <li><code>id</code> (int): Token ID</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Token string</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If ID is not in vocabulary</li> </ul>"},{"location":"api/#token_to_id","title":"token_to_id","text":"<pre><code>def token_to_id(self, token: str) -&gt; int\n</code></pre> <p>Convert token string to token ID.</p> <p>Arguments:</p> <ul> <li><code>token</code> (str): Token string</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: Token ID</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If token is not in vocabulary</li> </ul>"},{"location":"api/#properties","title":"Properties","text":"Property Type Description <code>vocab_size</code> int Total vocabulary size (special + base atoms + merges) <code>base_vocab_size</code> int Number of base atom tokens <code>num_merges</code> int Number of learned merge operations <code>pad_token_id</code> int PAD token ID (always 0) <code>unk_token_id</code> int UNK token ID (always 1) <code>bos_token_id</code> int BOS token ID (always 2) <code>eos_token_id</code> int EOS token ID (always 3) <code>pad_token</code> str PAD token string (<code>&lt;pad&gt;</code>) <code>unk_token</code> str UNK token string (<code>&lt;unk&gt;</code>) <code>bos_token</code> str BOS token string (<code>&lt;bos&gt;</code>) <code>eos_token</code> str EOS token string (<code>&lt;eos&gt;</code>)"},{"location":"quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with rustmolbpe.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install maturin\ngit clone https://github.com/HFooladi/rustmolbpe.git\ncd rustmolbpe\nmaturin develop --release\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#loading-a-pre-trained-tokenizer","title":"Loading a Pre-trained Tokenizer","text":"<pre><code>import rustmolbpe\n\n# Create tokenizer\ntokenizer = rustmolbpe.SmilesTokenizer()\n\n# Load pre-trained vocabulary\ntokenizer.load_vocabulary(\"data/chembl36_vocab.txt\")\n\nprint(f\"Vocabulary size: {tokenizer.vocab_size}\")\n</code></pre>"},{"location":"quickstart/#encoding-and-decoding","title":"Encoding and Decoding","text":"<pre><code># Encode a SMILES string\nsmiles = \"CC(=O)Nc1ccc(O)cc1\"  # paracetamol\nids = tokenizer.encode(smiles)\nprint(f\"Token IDs: {ids}\")\n\n# Decode back to SMILES\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\nassert decoded == smiles\n</code></pre>"},{"location":"quickstart/#using-special-tokens","title":"Using Special Tokens","text":"<pre><code># Encode with BOS/EOS tokens\nids = tokenizer.encode(\"CCO\", add_special_tokens=True)\n# Result: [2, ..., 3]  where 2=BOS, 3=EOS\n\n# Special token IDs are fixed:\nprint(f\"PAD: {tokenizer.pad_token_id}\")  # 0\nprint(f\"UNK: {tokenizer.unk_token_id}\")  # 1\nprint(f\"BOS: {tokenizer.bos_token_id}\")  # 2\nprint(f\"EOS: {tokenizer.eos_token_id}\")  # 3\n</code></pre>"},{"location":"quickstart/#batch-processing","title":"Batch Processing","text":""},{"location":"quickstart/#basic-batch-encoding","title":"Basic Batch Encoding","text":"<pre><code>smiles_list = [\"CCO\", \"c1ccccc1\", \"CC(=O)O\"]\n\n# Parallel batch encoding\nids_list = tokenizer.batch_encode(smiles_list)\n\n# Batch decoding\ndecoded_list = tokenizer.batch_decode(ids_list)\n</code></pre>"},{"location":"quickstart/#padded-batches-for-ml","title":"Padded Batches for ML","text":"<p>For training neural networks, you need padded sequences:</p> <pre><code>result = tokenizer.encode_batch_padded(\n    smiles_list,\n    max_length=20,           # Pad/truncate to this length\n    padding=\"right\",          # Pad on the right\n    truncation=True,          # Truncate if longer\n    add_special_tokens=True,  # Add BOS/EOS\n    return_attention_mask=True\n)\n\ninput_ids = result[\"input_ids\"]       # Padded token IDs\nattention_mask = result[\"attention_mask\"]  # 1 for real tokens, 0 for padding\n</code></pre>"},{"location":"quickstart/#training-your-own-tokenizer","title":"Training Your Own Tokenizer","text":"<pre><code>def smiles_generator(filepath):\n    \"\"\"Generator for streaming large files.\"\"\"\n    with open(filepath) as f:\n        for line in f:\n            yield line.strip()\n\n# Create and train\ntokenizer = rustmolbpe.SmilesTokenizer()\ntokenizer.train_from_iterator(\n    smiles_generator(\"molecules.smi\"),\n    vocab_size=8000,\n    buffer_size=16384,\n    min_frequency=2\n)\n\n# Save vocabulary\ntokenizer.save_vocabulary(\"my_vocab.txt\")\n</code></pre>"},{"location":"quickstart/#atom-level-tokenization","title":"Atom-level Tokenization","text":"<p>For inspection or custom processing:</p> <pre><code>atoms = rustmolbpe.atomwise_tokenize(\"c1ccccc1\")\nprint(atoms)  # ['c', '1', 'c', 'c', 'c', 'c', 'c', '1']\n\natoms = rustmolbpe.atomwise_tokenize(\"[C@@H](O)C\")\nprint(atoms)  # ['[C@@H]', '(', 'O', ')', 'C']\n</code></pre>"},{"location":"quickstart/#vocabulary-inspection","title":"Vocabulary Inspection","text":"<pre><code># Get all tokens\nvocab = tokenizer.get_vocabulary()\nfor token, token_id in vocab[:10]:\n    print(f\"ID {token_id}: '{token}'\")\n\n# Lookup\ntoken_id = tokenizer.token_to_id(\"CC\")\ntoken = tokenizer.id_to_token(token_id)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>See API Reference for complete documentation</li> <li>Check <code>examples/</code> directory for more detailed examples</li> </ul>"}]}